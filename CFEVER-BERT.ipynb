{"cells":[{"cell_type":"markdown","source":["### **Initialisation**"],"metadata":{"id":"N6JbKZxBlkMB"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19314,"status":"ok","timestamp":1684143785415,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"},"user_tz":-600},"id":"m2x1AxdIIZdV","outputId":"5037d9df-a830-430c-8583-a08e3fcee076"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gFO2q9vxIySV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"353f83fb-9dec-43b8-d6ec-c9f7608a03b0","executionInfo":{"status":"ok","timestamp":1684143798795,"user_tz":-600,"elapsed":13382,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n","Collecting transformers\n","  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"]}],"source":["!pip install torch torchvision transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IwTt7gVzIq4n","executionInfo":{"status":"ok","timestamp":1684143808563,"user_tz":-600,"elapsed":9770,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["import json\n","import math\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import random\n","import transformers\n","from transformers import BertTokenizer\n","from transformers import BertModel\n","from collections import Counter, defaultdict\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","from transformers import AdamW\n","import time\n","import copy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from torch.nn import functional as F\n","from collections import Counter"]},{"cell_type":"markdown","metadata":{"id":"XLoOzgD5IM-V"},"source":["Load the data and specify the paths:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kAT8b8HxIH8H","executionInfo":{"status":"ok","timestamp":1684143814646,"user_tz":-600,"elapsed":6086,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["transformers.logging.set_verbosity_error()\n","path_prefix = '/content/drive/MyDrive/Colab Notebooks/Assignment3/' # <!IMPORTANT!> Please replace with your own path\n","\n","train_path = path_prefix + 'project-data/train-claims.json'  \n","dev_path = path_prefix + 'project-data/dev-claims.json'\n","test_path = path_prefix + 'project-data/test-claims-unlabelled.json'\n","evidence_path = path_prefix + 'project-data/evidence.json'\n","\n","train_claims = json.load(open(train_path))\n","dev_claims = json.load(open(dev_path))\n","test_claims = json.load(open(test_path))\n","evidences = json.load(open(evidence_path))\n","\n","#dev_train_claims = {**dict(train_claims), **dict(dev_claims)}"]},{"cell_type":"markdown","source":["### **Evidence Retrieval - Function Declarations**"],"metadata":{"id":"OAWnhCoyEIf0"}},{"cell_type":"code","source":["random.seed(42)\n","evidence_key_prefix = 'evidence-'\n","er_result_filename = path_prefix + \"evidence-retrieval-only-results.json\"\n","er_model_params_filename = path_prefix + 'cfeverercls.dat'\n","claim_hard_negatives_filename = path_prefix + 'claim-hard-negative-evidences.json'\n","\n","# ----------Hyperparameters of the entire pipeline----------\n","# --------------Evidence Retrieval--------------\n","d_bert_base = 768\n","gpu = 0\n","input_seq_max_len = 384\n","er_pos_neg_sample_ratio = 5\n","train_neg_cand_num = 5000\n","pre_select_evidence_num = 1000\n","loader_batch_size = 16\n","loader_worker_num = 2\n","num_epoch_pre = 1\n","num_epoch_hne = 13\n","hnm_threshold = 0.5\n","hnm_batch_size = 12\n","evidence_selection_threshold = 0.9\n","max_evi = 5\n","opti_lr_er_pre = 2e-5\n","opti_lr_er_hne = 2e-7\n","grad_step_period_pre = 4\n","grad_step_period_hne = 4\n","# ----------------------------------------------"],"metadata":{"id":"lF-dRYHNEGch","executionInfo":{"status":"ok","timestamp":1684143814646,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkPjaAWwNK4X"},"source":["Define Dataset for Evidence Retrieval:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"rv_SokT_UyyE","executionInfo":{"status":"ok","timestamp":1684143814646,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["class CFEVERERTrainDataset(Dataset):\n","    \"\"\"Climate Fact Extraction and Verification Dataset for Train, for the Evidence Retrieval task.\"\"\"\n","\n","    def __init__(self, claims, evidences_, tokenizer, max_len=input_seq_max_len, sample_ratio=er_pos_neg_sample_ratio, train_neg_cand_num=train_neg_cand_num):\n","        self.data_set = unroll_train_claim_evidences(claims, evidences_, sample_ratio=sample_ratio, train_neg_cand_num=train_neg_cand_num)\n","        self.max_len = max_len\n","        self.claims = claims\n","        self.evidences = evidences_\n","        self.sample_ratio = sample_ratio\n","        self.train_neg_cand_num = train_neg_cand_num\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def reset_data_hne(self, claim_hard_negative_evidences):\n","        self.data_set = unroll_train_claim_evidences_with_hne(self.claims, self.evidences, claim_hard_negative_evidences)\n","\n","    def __getitem__(self, index):\n","        claim_id, evidence_id, label = self.data_set[index]\n","\n","        # Preprocessing the text to be suitable for BERT\n","        claim_evidence_in_tokens = self.tokenizer.encode_plus(self.claims[claim_id]['claim_text'], self.evidences[evidence_id], \n","                                                              return_tensors='pt', padding='max_length', truncation=True,\n","                                                              max_length=self.max_len, return_token_type_ids=True)\n","        \n","        seq, attn_masks, segment_ids = claim_evidence_in_tokens['input_ids'].squeeze(0), claim_evidence_in_tokens[\n","                'attention_mask'].squeeze(0), claim_evidence_in_tokens['token_type_ids'].squeeze(0)\n","    \n","        return seq, attn_masks, segment_ids, label"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KeEee40ld1yn","executionInfo":{"status":"ok","timestamp":1684143814647,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["class CFEVERERTestDataset(Dataset):\n","    \"\"\"Climate Fact Extraction and Verification Dataset for Dev/Test, for the Evidence Retrieval task.\"\"\"\n","\n","    def __init__(self, claims, evidences_, tokenizer, max_len=input_seq_max_len, max_candidates=pre_select_evidence_num):\n","        self.data_set = unroll_test_claim_evidences(claims, evidences_, max_candidates=max_candidates)\n","        self.max_len = max_len\n","        self.claims = claims\n","        self.evidences = evidences_\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, index):\n","        claim_id, evidence_id = self.data_set[index]\n","\n","        # Preprocessing the text to be suitable for BERT\n","        claim_evidence_in_tokens = self.tokenizer.encode_plus(self.claims[claim_id]['claim_text'], self.evidences[evidence_id], \n","                                                              return_tensors='pt', padding='max_length', truncation=True,\n","                                                              max_length=self.max_len, return_token_type_ids=True)\n","        \n","        seq, attn_masks, segment_ids = claim_evidence_in_tokens['input_ids'].squeeze(0), claim_evidence_in_tokens[\n","                'attention_mask'].squeeze(0), claim_evidence_in_tokens['token_type_ids'].squeeze(0)\n","    \n","        return seq, attn_masks, segment_ids, claim_id, evidence_id"]},{"cell_type":"markdown","metadata":{"id":"orTyfIUaNHMZ"},"source":["Generate/Pre-select evidence candidates for train/test, that will be forwarded to the BERT-based classifer for processing:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Ptb_eG5Ayuo8","executionInfo":{"status":"ok","timestamp":1684143814647,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def unroll_train_claim_evidences(claims, evidences_, sample_ratio, train_neg_cand_num):\n","    \"\"\"\n","    This function aims to define the train evidences for each claim, \n","    unroll them into pairs, and return a list of claim-evidence pairs\n","    in the form of (claim_id, evidence_id, label).\n","\n","    Rule: Includes all the positive evidences for each claim, and randomly\n","    sample negative evidences for each claim, within the chosen top TF-IDF\n","    cosine similarity range. Number of negative evidences is determined by \n","    the sample_ratio.\n","    \"\"\"\n","    st = time.time()\n","\n","    vectorizer = TfidfVectorizer(stop_words='english')\n","    vectorizer.fit(list(evidences_.values()) + [claims[c][\"claim_text\"] for c in claims])\n","    evidences_tfidf = vectorizer.transform(evidences_.values())\n","\n","    train_claim_evidence_pairs = []\n","\n","    for cid in claims:\n","        claim_tfidf = vectorizer.transform([claims[cid]['claim_text']])\n","\n","        for train_evidence_id, label in generate_train_evidence_samples(evidences_, claims[cid]['evidences'], claim_tfidf, evidences_tfidf, sample_ratio, train_neg_cand_num):\n","            train_claim_evidence_pairs.append((cid, train_evidence_id, label))\n","\n","    random.shuffle(train_claim_evidence_pairs)\n","    print(f\"Finished unrolling train claim-evidence pairs in {time.time() - st} seconds.\")\n","\n","    return train_claim_evidence_pairs"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Jubn_eW1eCuM","executionInfo":{"status":"ok","timestamp":1684143814648,"user_tz":-600,"elapsed":11,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def unroll_test_claim_evidences(claims, evidences_, max_candidates):\n","    \"\"\"\n","    This function aims to define the evidences to be further processed\n","    by the BERT model for each test claim. The evidences are unrolled\n","    into pairs, and return a list of claim-evidence pairs in the form\n","    of (claim_id, evidence_id).\n","\n","    Rule: Includes the top <max_candidates> evidences for each claim \n","    based on the TF-IDF cosine similarity score with the corresponding\n","    claim.\n","    \"\"\"\n","    st = time.time()\n","\n","    vectorizer = TfidfVectorizer(stop_words='english')\n","    vectorizer.fit(list(evidences_.values()) + [claims[c][\"claim_text\"] for c in claims])\n","    evidences_tfidf = vectorizer.transform(evidences_.values())\n","\n","    test_claim_evidence_pairs = []\n","    for cid in claims:\n","        claim_tfidf = vectorizer.transform([claims[cid]['claim_text']])\n","\n","        for test_evidence_id in generate_test_evidence_candidates(evidences_, evidences_tfidf, claim_tfidf, max_candidates):\n","            test_claim_evidence_pairs.append((cid, test_evidence_id))\n","\n","    print(f\"Finished unrolling test claim-evidence pairs in {time.time() - st} seconds.\")\n","\n","    return test_claim_evidence_pairs"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4xdnzWY7XyMC","executionInfo":{"status":"ok","timestamp":1684143814648,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def generate_train_evidence_samples(evidences_, claim_evidences, claim_tfidf, evidences_tfidf, sample_ratio, train_neg_cand_num):\n","    \"\"\"\n","    Generate training samples for a given claim for the evidence retrieval task.\n","    :param evidences_: the full evidence set.\n","    :param claim_evidences: the ground truth evidence set for the claim. In the form of a list of evidence ids\n","    :param claim_tfidf: the tfidf vector for the claim text\n","    :param evidences_tfidf: the tfidf vectors for the entire evidence set\n","    :param sample_ratio: the ratio of positive to negative samples: neg/pos\n","    :param train_neg_cand_num: the top TF-IDF cosine similarity range which the negative samples are chosen from\n","    :return: a list of evidence samples zipped with the corresponding labels. - (evi id, label)\n","    \"\"\"\n","    similarity = cosine_similarity(claim_tfidf, evidences_tfidf).squeeze()\n","    \n","    df = pd.DataFrame({\"evidences\": evidences_.keys(), \"similarity\": similarity}).sort_values(by=['similarity'], ascending=False)\n","    train_neg_candidates = df.iloc[:train_neg_cand_num][\"evidences\"].tolist()\n","    \n","    # Get positive samples\n","    samples = claim_evidences.copy()  # evidence ids\n","\n","    # Get negative samples\n","    while len(samples) < math.ceil(len(claim_evidences) * (sample_ratio + 1)):\n","        neg_sample = train_neg_candidates[random.randint(0, len(train_neg_candidates) - 1)]  # random selection\n","        \n","        if neg_sample not in samples:\n","            samples.append(neg_sample)\n","\n","    samples_with_labels = list(zip(samples, [1] * len(claim_evidences) + [0] * (len(samples) - len(claim_evidences))))\n","\n","    return samples_with_labels"]},{"cell_type":"code","source":["def generate_random_train_evidence_samples(evidences_, claim_evidences, sample_ratio):\n","    \"\"\"\n","    Generate training samples for each of the claims for the evidence retrieval task.\n","    :param evidences_: the full evidence set.\n","    :param claim_evidences: the ground truth evidence set for the claim. In the form of a list of evidence ids\n","    :param sample_ratio: the ratio of positive to negative samples: neg/pos\n","    :return: a list of evidence samples zipped with the corresponding labels. - (evi id, label)\n","    \"\"\"\n","        \n","    # Get positive samples\n","    samples = claim_evidences.copy()  # evidence ids\n","\n","    # Get negative samples\n","    while len(samples) < math.ceil(len(claim_evidences) * (sample_ratio + 1)):\n","        neg_sample = evidence_key_prefix + str(random.randint(0, len(evidences_) - 1))  # random selection\n","        \n","        if neg_sample not in samples:\n","            samples.append(neg_sample)\n","\n","    samples_with_labels = list(zip(samples, [1] * len(claim_evidences) + [0] * (len(samples) - len(claim_evidences))))\n","\n","    return samples_with_labels"],"metadata":{"id":"E6xHCuupzX1E","executionInfo":{"status":"ok","timestamp":1684143814648,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"grvEUhH5MU1K","executionInfo":{"status":"ok","timestamp":1684143814648,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def generate_test_evidence_candidates(evidences_, evidences_tfidf, claim_tfidf, max_candidates):\n","    \"\"\"\n","    :param evidences_: the full evidence set.\n","    :param evidences_tfidf: The tfidf matrix of the entire evidence set\n","    :param claim_tfidf: The tfidf vector of the query claim (also a matrix technically).\n","    :param max_candidates: Number of evidences to be selected for further processing.\n","    :return: a list of the selected evidences.\n","    \"\"\"\n","    similarity = cosine_similarity(claim_tfidf, evidences_tfidf).squeeze()\n","    \n","    df = pd.DataFrame({\"evidences\": evidences_.keys(), \"similarity\": similarity}).sort_values(by=['similarity'], ascending=False)\n","    potential_relevant_evidences = df.iloc[:max_candidates][\"evidences\"].tolist()\n","\n","    return potential_relevant_evidences"]},{"cell_type":"markdown","metadata":{"id":"pgdrDH8dNma_"},"source":["Evidence Retrieval Model (Claim-Evidence Pair Classifier):"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3G7Ci3UicaJx","executionInfo":{"status":"ok","timestamp":1684143814648,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["class CFEVERERClassifier(nn.Module):\n","    def __init__(self):\n","        super(CFEVERERClassifier, self).__init__()\n","\n","        # Instantiating BERT model object\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Classification layer\n","        # input dimension is 768 because [CLS] embedding has a dimension of 768, if bert base is used\n","        # output dimension is 1 because we're working with a binary classification problem - RELEVANT : NOT RELEVANT\n","        self.cls_layer = nn.Linear(d_bert_base, 1)\n","\n","    def forward(self, seq, attn_masks, segment_ids):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","            -segment_ids : Tensor of shape [B, T] containing token ids of segment embeddings (see BERT paper for more details)\n","        '''\n","        \n","        # Feeding the input to BERT model to obtain contextualized representations\n","        outputs = self.bert(seq, attention_mask=attn_masks, token_type_ids=segment_ids, return_dict=True)\n","        cont_reps = outputs.last_hidden_state\n","\n","        # Obtaining the representation of [CLS] head (the first token)\n","        cls_rep = cont_reps[:, 0]\n","\n","        # Feeding cls_rep to the classifier layer\n","        logits = self.cls_layer(cls_rep)\n","\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"v2br3WM8NuuX"},"source":["Train the Claim-Evidence Pair Classifier:"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6Y10vdEfmAiA","executionInfo":{"status":"ok","timestamp":1684143814649,"user_tz":-600,"elapsed":11,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def train_evi_retrieval(net, loss_criterion, opti, train_loader, dev_loader, dev_claims, gpu, max_eps, grad_step_period):\n","    best_f1 = 0\n","    mean_losses = [0] * max_eps\n","    \n","    for ep in range(max_eps):\n","        net.train()  # Good practice to set the mode of the model\n","        st = time.time()\n","        opti.zero_grad()\n","        count = 0\n","        train_acc = 0\n","        \n","        for i, (seq, attn_masks, segment_ids, labels) in enumerate(train_loader):\n","            # Extracting the tokens ids, attention masks and token type ids\n","            seq, attn_masks, segment_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), segment_ids.cuda(gpu), labels.cuda(gpu)\n","\n","            # Obtaining the logits from the model\n","            logits = net(seq, attn_masks, segment_ids)\n","\n","            # Computing loss\n","            loss = loss_criterion(logits.squeeze(-1), labels.float())\n","\n","            mean_losses[ep] += loss.item()\n","            count += 1\n","            train_acc += get_accuracy_from_logits(logits, labels)\n","\n","            scaled_loss = loss / grad_step_period  # normalise loss, scale to larger batch size, as original batch size cannot be handled due to GPU limitation\n","\n","            # Backpropagating the gradients, account for gradients\n","            scaled_loss.backward()\n","\n","            if (i + 1) % grad_step_period == 0:\n","                # Optimization step, apply the gradients\n","                opti.step()\n","\n","                # Reset/Clear gradients\n","                opti.zero_grad()\n","\n","            if i % 100 == 0:\n","                print(\"Iteration {} of epoch {} complete. Time taken (s): {}\".format(i, ep, (time.time() - st)))\n","                st = time.time()\n","        \n","        mean_losses[ep] /= count\n","        print(f\"Epoch {ep} completed. Loss: {mean_losses[ep]}, Accuracy: {train_acc / count}.\\n\")\n","        \n","        if (ep + 1) % 1 == 0:\n","            dev_st = time.time()\n","            print(\"Evaluating on the dev set... (This might take a while)\")\n","            f1, recall, precision, dev_loss = evaluate(net, dev_loader, dev_claims, loss_criterion, gpu)\n","            print(\"\\nEpoch {} completed! Evaluation on dev set took {} seconds.\\nDevelopment F1: {}; Development Recall: {}; Development Precision: {}; Dev Loss: {}\".format(ep, time.time() - dev_st, f1, recall, precision, dev_loss))\n","            \n","            if f1 > best_f1:\n","                print(\"Best development f1 improved from {} to {}, saving model...\\n\".format(best_f1, f1))\n","                best_f1 = f1\n","                torch.save(net.state_dict(), er_model_params_filename)\n","            else:\n","                print()\n","    \n","    return mean_losses"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"6BVJkQjmMKfG","executionInfo":{"status":"ok","timestamp":1684143814649,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def get_accuracy_from_logits(logits, labels):\n","    probs = torch.sigmoid(logits.unsqueeze(-1))\n","    preds = (probs > 0.5).long()\n","    acc = (preds.squeeze() == labels).float().mean()\n","    return acc\n","\n","def get_probs_from_logits(logits):\n","    probs = torch.sigmoid(logits.unsqueeze(-1))\n","\n","    return probs.squeeze()"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"U_7ohNtBAqtR","executionInfo":{"status":"ok","timestamp":1684143814649,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def select_evi_df(df, threshold, max_evidences):\n","    \"\"\"\n","    Selects the top <max_evidences> evidences from the \n","    dataframe <df> with a probability higher than <threshold>.\n","    If no one satisifies the threshold, the evidence with the highest\n","    probability is selected.\n","    \"\"\"\n","    \n","    max_prob_evi = df[df['probs'] == df['probs'].max()]\n","\n","    df = df[df['probs'] > threshold].nlargest(max_evidences, \"probs\")\n","\n","    if len(df) == 0:\n","        df = max_prob_evi\n","\n","    return df\n","\n","def predict_evi(net, dataloader, gpu, threshold=evidence_selection_threshold, max_evidences=max_evi, evaluate=False, evaluation_claims=None, loss_criterion=None):\n","    net.eval()\n","\n","    claim_evidences = defaultdict(list)\n","    df = pd.DataFrame()\n","    mean_loss = 0\n","\n","    with torch.no_grad():  # suspend grad track, save time and memory\n","        for seq, attn_masks, segment_ids, claim_ids, evidence_ids in dataloader:\n","            if evaluate and evaluation_claims is not None:\n","                labels = torch.tensor([1 if evidence_ids[i] in evaluation_claims[claim_ids[i]]['evidences'] else 0 for i in range(len(claim_ids))])\n","                labels = labels.cuda(gpu)\n","\n","            seq, attn_masks, segment_ids = seq.cuda(gpu), attn_masks.cuda(gpu), segment_ids.cuda(gpu)\n","            logits = net(seq, attn_masks, segment_ids)\n","            probs = get_probs_from_logits(logits)\n","\n","            if evaluate:\n","                mean_loss += loss_criterion(logits.squeeze(-1), labels.float()).item()\n","            \n","            df = pd.concat([df, pd.DataFrame({\"claim_ids\": claim_ids, \"evidence_ids\": evidence_ids, \"probs\": probs.cpu()})], ignore_index=True)\n","\n","    # groupby gives a df for each claim_ids, then for each df, apply() the selection, finally reset_index to get rid of the multi-index\n","    filtered_claim_evidences_df = df.groupby('claim_ids').apply(lambda x: select_evi_df(x, threshold, max_evidences)).reset_index(drop=True)\n","\n","    # with open(path_prefix + 'pred_probabilities.json', 'w') as f:\n","    #     json.dump(filtered_claim_evidences_df['probs'].to_dict(), f)\n","\n","    for _, row in filtered_claim_evidences_df.iterrows():\n","        claim_id = row['claim_ids']\n","        evidence_id = row['evidence_ids']\n","\n","        claim_evidences[claim_id].append(evidence_id)\n","    \n","    return claim_evidences if not evaluate else (claim_evidences, mean_loss / len(dataloader))"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"T6F6cSXPgOre","executionInfo":{"status":"ok","timestamp":1684143814649,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def evaluate(net, dataloader, dev_claims, loss_criterion, gpu):\n","    \"\"\"\n","    Used to evaluate the dev set performance of the model.\n","    \"\"\"\n","    claim_evidences, loss = predict_evi(net, dataloader, gpu, evaluate=True, evaluation_claims=dev_claims, loss_criterion=loss_criterion)\n","\n","    fscores, recalls, precisions = [], [], []\n","\n","    for claim_id, evidences in claim_evidences.items():\n","        e_true = dev_claims[claim_id]['evidences']\n","        recall = len([e for e in evidences if e in e_true]) / len(e_true)\n","        precision = len([e for e in evidences if e in e_true]) / len(evidences)\n","        fscore = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0.0\n","\n","        fscores.append(fscore)\n","        precisions.append(precision)\n","        recalls.append(recall)\n","\n","    mean_f = np.mean(fscores if len(fscores) > 0 else [0.0])\n","    mean_recall = np.mean(recalls if len(recalls) > 0 else [0.0])\n","    mean_precision = np.mean(precisions if len(precisions) > 0 else [0.0])\n","\n","    return mean_f, mean_recall, mean_precision, loss  # F1 Score, recall, precision, loss"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"TtjQmIy0MOEj","executionInfo":{"status":"ok","timestamp":1684143814649,"user_tz":-600,"elapsed":10,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def extract_er_result(claim_evidences, claims, filename=er_result_filename):\n","    \"\"\"\n","    Extract the evidences from the claim_evidences dict and\n","    save the result to a json file. This step only considers\n","    the evidences for a claim, with no care to the labels.\n","    \"\"\"\n","    extracted_claims = copy.deepcopy(claims)\n","\n","    for c in extracted_claims:\n","        extracted_claims[c][\"evidences\"] = claim_evidences[c]\n","    \n","    with open(filename, 'w') as f:\n","        json.dump(extracted_claims, f)\n","\n","    return extracted_claims"]},{"cell_type":"markdown","source":["Create the evidence retrieval classifier and optimizer:"],"metadata":{"id":"2rP3Z8e9lV4v"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"nPe6y6AjQT_z","executionInfo":{"status":"ok","timestamp":1684143821578,"user_tz":-600,"elapsed":6939,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}},"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["d5f8aa6ac64d4250b4c970e2e44710bf","ed732b4df94b49fe87ad7517ee44f711","0bfb603807b44e77ae8b0fc97408c678","b3fb05286ae34af1ae2b7a58690e47f9","432657e0c65c46a384e001e307761cde","12131d25b19d4eb189b3343ec7aef74c","dddd9ae8ebb34cea99543d2bccada370","5bed5c487b6247088ccc15bf5b8c1618","a6dde0e00ecc468fbb493243bac2e302","55df7111524d4e588e82e04519ae0f01","16177e50dd164c318aff70ab0c873790","055934f932724136b887bc058dbe6c5d","821ffe1137e44e9f8c870a272de4e4ce","755725ae4e3e49d1a700b993aff4dada","228a374a85b8467d94ac93d2c7f554d8","bdd9c9b2edcd493798bebdcc467db72c","2fa980f3162f4022ae04c1f2d26c8ee9","90e26c7cf5c446ab88a58d4c883e330b","868a1c7c37f14047ab933a71c0c711ae","e9459649412942afa976eb59a9dd105a","b192a06cddde4b25993035590e4963dd","ac5af82a4cbb44628c92beac868b79b9","302c6088874c479794d4ebbcc6c8626c","849e55240f914a2baf69fe7cc96d7b47","fb00b70c0dd44356b0c33465870e86a2","22a5d51f83b34f659effb646f053938e","5a97613001b845ab95f0416e253cfb5a","67b8a743c6db4e419346d17c2c492f32","7b3a68edd4904e269163144038e5f224","f13259a798124913a6cf9b3c7de47952","70a672b5e8d6444a8837b13763ba31a1","cfe6367e4bb445d580b0d6d742bd5943","bf654a93dc134c27b13adc88bd3488dd","f44557afc58544879652587334309b83","1cb9e6592a674e0fb180413aec800865","3d4e34f20ba647f09312420f90090903","26eac9f552e546eaa6245d9a52960e35","c5d982e03c3e4341aa2d225f3c542102","0635dc12c8e6410cbe0f40d8b599c89e","8f5d1be7d02044e0b52787247edd8b3e","f099b02528734f0d99a049741ce4ff39","3c7dab5b9ca34421bdc545bf9f0d4a0c","7da03bf37767403ea368474ddaeb1ccf","64941ef4cf63481eb7338b6999cfaec3"]},"outputId":"35cb7e35-c099-4d06-c540-1f9ffb03892e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5f8aa6ac64d4250b4c970e2e44710bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055934f932724136b887bc058dbe6c5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302c6088874c479794d4ebbcc6c8626c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f44557afc58544879652587334309b83"}},"metadata":{}}],"source":["net_er = CFEVERERClassifier()\n","net_er.cuda(gpu) #Enable gpu support for the model\n","\n","loss_criterion = nn.BCEWithLogitsLoss(weight=torch.tensor([er_pos_neg_sample_ratio])).cuda(gpu)\n","opti_er_pre = optim.Adam(net_er.parameters(), lr=opti_lr_er_pre)\n","\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","source":["### **Evidence Retrieval - Training**\n","\n","This phrase trains the BERT model on claim-evidence pairs with all positive evidences for a claim and the same number of random negative evidences. For the pre-defined number of epochs. Evaluated on the dev set for each epoch, the best one will be chosen for HNM later and further fine-tuning."],"metadata":{"id":"QvA3TgKsjYSl"}},{"cell_type":"markdown","metadata":{"id":"mKhyVW_ZODBK"},"source":["Create the respective data loaders:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gK2P43qMAqxt"},"outputs":[],"source":["# Creating instances of training and development set\n","train_set = CFEVERERTrainDataset(train_claims, evidences, bert_tokenizer)\n","dev_set = CFEVERERTestDataset(dev_claims, evidences, bert_tokenizer, max_candidates=100)\n","\n","#Creating intsances of training and development dataloaders\n","train_loader = DataLoader(train_set, batch_size=loader_batch_size, num_workers=loader_worker_num)\n","dev_loader = DataLoader(dev_set, batch_size=loader_batch_size, num_workers=loader_worker_num)"]},{"cell_type":"markdown","metadata":{"id":"mngXF40VqQIV"},"source":["Do training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTbzCUr1Aq1v","executionInfo":{"status":"aborted","timestamp":1684143913611,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["# First phrase: pre-train the model on all positive claim-evidence pairs and same number of random negative pairs\n","train_evi_retrieval(net_er, loss_criterion, opti_er_pre, train_loader, dev_loader, dev_claims, gpu, num_epoch_pre, grad_step_period_pre)"]},{"cell_type":"markdown","metadata":{"id":"EG0vd_xl4FI0"},"source":["### **Evidence Retrieval Baseline Model (TFIDF)**"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"-k010jud4YUJ","executionInfo":{"status":"ok","timestamp":1684143919689,"user_tz":-600,"elapsed":725,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"outputs":[],"source":["def tfidf_cos_baseline(claims, evidences, evidence_select_num=5):\n","    \"\"\"\n","    Selects the K most cosine similar evidences based on TF-IDF.\n","    \"\"\"\n","    fscores, recalls, precisions = [], [], []\n","    claim_evidences = {}\n","\n","    vectorizer = TfidfVectorizer(stop_words='english')\n","    vectorizer.fit(list(evidences.values()) + [claims[c][\"claim_text\"] for c in claims])\n","    evidences_tfidf = vectorizer.transform(evidences.values())\n","\n","    for c in claims:\n","        claim_tfidf = vectorizer.transform([claims[c][\"claim_text\"]])\n","\n","        cos_sims = cosine_similarity(claim_tfidf, evidences_tfidf).squeeze()\n","    \n","        df = pd.DataFrame({\"evidences\": evidences.keys(), \"similarity\": cos_sims}).sort_values(by=['similarity'], ascending=False)\n","        claim_evidences[c] = df.iloc[:evidence_select_num][\"evidences\"].tolist()\n","    \n","    for claim_id, evidences in claim_evidences.items():\n","        e_true = claims[claim_id]['evidences']\n","        recall = len([e for e in evidences if e in e_true]) / len(e_true)\n","        precision = len([e for e in evidences if e in e_true]) / len(evidences)\n","        fscore = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0.0\n","\n","        fscores.append(fscore)\n","        precisions.append(precision)\n","        recalls.append(recall)\n","\n","    mean_f = np.mean(fscores if len(fscores) > 0 else [0.0])\n","    mean_recall = np.mean(recalls if len(recalls) > 0 else [0.0])\n","    mean_precision = np.mean(precisions if len(precisions) > 0 else [0.0])\n","\n","    return mean_f, mean_recall, mean_precision  # F1 Score, recall, precision"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"55u9neYp6w92","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684144105033,"user_tz":-600,"elapsed":184805,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}},"outputId":"b4c052a0-f496-44e0-9962-75802097f9ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["------Evidence Retrival Baseline Performance------\n","F1-Score: 0.09012059369202229\n","Recall-Score: 0.14469696969696969\n","Precision-Score: 0.07272727272727274\n","--------------------------------------------------\n"]}],"source":["f1, recall, precision = tfidf_cos_baseline(dev_claims, evidences)\n","print(\"------Evidence Retrival Baseline Performance------\")\n","print(f\"F1-Score: {f1}\")\n","print(f\"Recall-Score: {recall}\")\n","print(f\"Precision-Score: {precision}\")\n","print(\"--------------------------------------------------\")"]},{"cell_type":"markdown","source":["### **Claim Label Classification - Function Declarations**"],"metadata":{"id":"wg6GXDB3DzY2"}},{"cell_type":"code","source":["clc_model_params_filename = path_prefix + 'cfeverlabelcls.dat'\n","\n","# ----------Hyperparameters of the entire pipeline----------\n","# --------------Claim Label Classification--------------\n","d_bert_base = 768\n","gpu = 0\n","input_seq_max_len = 384\n","loader_batch_size = 16\n","loader_worker_num = 2\n","num_epoch = 9\n","num_of_classes = 4\n","opti_lr_clc = 2e-5\n","label_mapper_ltoi = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2}\n","label_mapper_itol = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO'}\n","# ------------------------------------------------------"],"metadata":{"id":"TbKDe_2YD2ya","executionInfo":{"status":"ok","timestamp":1684144105034,"user_tz":-600,"elapsed":5,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class CFEVERLabelTrainDataset(Dataset):\n","    \"\"\"Climate Fact Extraction and Verification Dataset for Training, for the Evidence Retrival task.\"\"\"\n","\n","    def __init__(self, claims, evidences_, max_len=input_seq_max_len):\n","        self.data_set = unroll_train_claim_evidence_pairs(claims)\n","        self.max_len = max_len\n","        self.claims = claims\n","        self.evidences = evidences_\n","\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, index):\n","        claim_id, evidence_id, label = self.data_set[index]\n","\n","        # Preprocessing the text to be suitable for BERT\n","        claim_evidence_in_tokens = self.tokenizer.encode_plus(self.claims[claim_id]['claim_text'], self.evidences[evidence_id], \n","                                                              return_tensors='pt', padding='max_length', truncation=True,\n","                                                              max_length=self.max_len, return_token_type_ids=True)\n","        \n","        seq, attn_masks, segment_ids = claim_evidence_in_tokens['input_ids'].squeeze(0), claim_evidence_in_tokens[\n","                'attention_mask'].squeeze(0), claim_evidence_in_tokens['token_type_ids'].squeeze(0)\n","    \n","        return seq, attn_masks, segment_ids, label"],"metadata":{"id":"jrup-JSnEV9E","executionInfo":{"status":"ok","timestamp":1684144105034,"user_tz":-600,"elapsed":5,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def unroll_train_claim_evidence_pairs(claims):\n","    \"\"\"\n","    Rule: \n","    Current approach considers all evidences to be with the \n","    label that the associated claim has, except for the DISPUTED label.\n","    \"\"\"\n","    claim_evidence_pairs = []\n","\n","    for claim_id in claims:\n","        if claims[claim_id]['claim_label'] != 'DISPUTED':\n","            for evidence_id in claims[claim_id]['evidences']:\n","                claim_evidence_pairs.append((claim_id, evidence_id, label_mapper_ltoi[claims[claim_id]['claim_label']]))\n","    \n","    return claim_evidence_pairs"],"metadata":{"id":"cmSlFiWrEXuc","executionInfo":{"status":"ok","timestamp":1684144105034,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class CFEVERLabelTestDataset(Dataset):\n","    \"\"\"Climate Fact Extraction and Verification Dataset for Testing, for the Evidence Retrival task.\"\"\"\n","\n","    def __init__(self, claims, evidences_, max_len=input_seq_max_len):\n","        self.data_set = unroll_test_claim_evidence_pairs(claims)\n","        self.max_len = max_len\n","        self.claims = claims\n","        self.evidences = evidences_\n","\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, index):\n","        claim_id, evidence_id = self.data_set[index]\n","\n","        # Preprocessing the text to be suitable for BERT\n","        claim_evidence_in_tokens = self.tokenizer.encode_plus(self.claims[claim_id]['claim_text'], self.evidences[evidence_id], \n","                                                              return_tensors='pt', padding='max_length', truncation=True,\n","                                                              max_length=self.max_len, return_token_type_ids=True)\n","        \n","        seq, attn_masks, segment_ids = claim_evidence_in_tokens['input_ids'].squeeze(0), claim_evidence_in_tokens[\n","                'attention_mask'].squeeze(0), claim_evidence_in_tokens['token_type_ids'].squeeze(0)\n","    \n","        return seq, attn_masks, segment_ids, claim_id"],"metadata":{"id":"JOAWOsOxEdoL","executionInfo":{"status":"ok","timestamp":1684144105034,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def unroll_test_claim_evidence_pairs(claims):\n","    claim_evidence_pairs = []\n","\n","    for claim_id in claims:\n","        for evidence_id in claims[claim_id]['evidences']:\n","            claim_evidence_pairs.append((claim_id, evidence_id))\n","    \n","    return claim_evidence_pairs"],"metadata":{"id":"6S7Fa-ErEfCj","executionInfo":{"status":"ok","timestamp":1684144105034,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class CFEVERLabelClassifier(nn.Module):\n","    def __init__(self):\n","        super(CFEVERLabelClassifier, self).__init__()\n","\n","        # Instantiating BERT model object\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Classification layer\n","        # input dimension is 768 because [CLS] embedding has a dimension of 768, if bert base is used\n","        # output dimension is 1 because we're working with a binary classification problem - RELEVANT : NOT RELEVANT\n","        self.cls_layer = nn.Linear(d_bert_base, num_of_classes)\n","\n","    def forward(self, seq, attn_masks, segment_ids):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","            -segment_ids : Tensor of shape [B, T] containing token ids of segment embeddings (see BERT paper for more details)\n","        '''\n","        \n","        # Feeding the input to BERT model to obtain contextualized representations\n","        outputs = self.bert(seq, attention_mask=attn_masks, token_type_ids=segment_ids, return_dict=True)\n","        cont_reps = outputs.last_hidden_state\n","\n","        # Obtaining the representation of [CLS] head (the first token)\n","        cls_rep = cont_reps[:, 0]\n","\n","        # Feeding cls_rep to the classifier layer\n","        logits = self.cls_layer(cls_rep)\n","\n","        return logits  # logits shape is [B, num_of_classes]"],"metadata":{"id":"txyf31hkEf-X","executionInfo":{"status":"ok","timestamp":1684144105034,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def train_claim_cls(net, loss_criterion, opti, train_loader, dev_loader, dev_claims, gpu, max_eps=num_epoch):\n","    best_acc = 0\n","    mean_losses = [0] * max_eps\n","\n","    for ep in range(max_eps):\n","        net.train()  # Good practice to set the mode of the model\n","        st = time.time()\n","        train_acc = 0\n","        count = 0\n","        \n","        for i, (b_seq, b_attn_masks, b_segment_ids, b_label) in enumerate(train_loader):\n","            # Reset/Clear gradients\n","            opti.zero_grad()\n","\n","            # Extracting the tokens ids, attention masks and token type ids\n","            b_seq, b_attn_masks, b_segment_ids, b_label = b_seq.cuda(gpu), b_attn_masks.cuda(gpu), b_segment_ids.cuda(gpu), b_label.cuda(gpu)\n","\n","            # Obtaining the logits from the model\n","            logits = net(b_seq, b_attn_masks, b_segment_ids)\n","\n","            # Computing loss\n","            loss = loss_criterion(logits, b_label)\n","\n","            mean_losses[ep] += loss.item()\n","            count += 1\n","            train_acc += get_accuracy_from_logits(logits, b_label)\n","\n","            # Backpropagating the gradients, account for gradients\n","            loss.backward()\n","\n","            # Optimization step, apply the gradients\n","            opti.step()\n","\n","            if i % 100 == 0:\n","                print(\"Iteration {} of epoch {} complete. Time taken (s): {}\".format(i, ep, (time.time() - st)))\n","                st = time.time()\n","        \n","        mean_losses[ep] /= count\n","        print(f\"Epoch {ep} completed. Loss: {mean_losses[ep]}, Accuracy: {train_acc / count}.\\n\")\n","\n","        dev_acc = evaluate_dev(net, dev_loader, dev_claims, gpu)\n","        print(\"\\nEpoch {} complete! Development Accuracy on dev claim labels: {}.\".format(ep, dev_acc))\n","        if dev_acc > best_acc:\n","            print(\"Best development accuracy improved from {} to {}, saving model...\\n\".format(best_acc, dev_acc))\n","            best_acc = dev_acc\n","            torch.save(net.state_dict(), clc_model_params_filename)\n","        else:\n","            print()\n","    \n","    return mean_losses"],"metadata":{"id":"jCSgpxCjEiIX","executionInfo":{"status":"ok","timestamp":1684144105035,"user_tz":-600,"elapsed":5,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def get_accuracy_from_logits(logits, labels):\n","    probs = F.softmax(logits, dim=-1)\n","    predicted_classes = torch.argmax(probs, dim=1)\n","    acc = (predicted_classes.squeeze() == labels).float().mean()\n","    return acc\n","\n","def get_predictions_from_logits(logits):\n","    probs = F.softmax(logits, dim=-1)\n","    predicted_classes = torch.argmax(probs, dim=1)\n","    return predicted_classes.squeeze()"],"metadata":{"id":"GjsZ6rzWEjiv","executionInfo":{"status":"ok","timestamp":1684144105035,"user_tz":-600,"elapsed":5,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def predict_pairs(net, dataloader, gpu):\n","    net.eval()\n","\n","    claim_evidence_labels = defaultdict(list)\n","    df = pd.DataFrame()\n","\n","    with torch.no_grad():\n","        for seq, attn_masks, segment_ids, claim_ids in dataloader:\n","            seq, attn_masks, segment_ids = seq.cuda(gpu), attn_masks.cuda(gpu), segment_ids.cuda(gpu)\n","            logits = net(seq, attn_masks, segment_ids)\n","            preds = get_predictions_from_logits(logits)\n","\n","            df = pd.concat([df, pd.DataFrame({\"claim_ids\": claim_ids, \"preds\": preds.cpu()})], ignore_index=True)\n","\n","    for _, row in df.iterrows():\n","        claim_id = row['claim_ids']\n","        label = row['preds']\n","\n","        claim_evidence_labels[claim_id].append(label)\n","    \n","    return claim_evidence_labels"],"metadata":{"id":"EHL4UOdyEliP","executionInfo":{"status":"ok","timestamp":1684144105035,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def decide_claim_labels_majority_vote(net, dataloader, gpu):\n","    \"\"\"\n","    This function decides the final label for each claim\n","    based on the designed rules.\n","\n","    Current Rule: Majority voting.\n","    \"\"\"\n","        \n","    claim_evidence_labels = predict_pairs(net, dataloader, gpu)\n","    claim_labels = {}\n","\n","    for claim_id in claim_evidence_labels:\n","        claim_labels[claim_id] = label_mapper_itol[Counter(claim_evidence_labels[claim_id]).most_common(1)[0][0]]  # label as the most common one - majority voting\n","    \n","    return claim_labels"],"metadata":{"id":"g3ZL_5xAEm03","executionInfo":{"status":"ok","timestamp":1684144105035,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def decide_claim_labels_rule_aggregation(net, dataloader, gpu):\n","    claim_evidence_labels = predict_pairs(net, dataloader, gpu)\n","    claim_labels = {}\n","\n","    for claim_id in claim_evidence_labels:\n","        if len(set(claim_evidence_labels[claim_id])) == 1:\n","            claim_labels[claim_id] = label_mapper_itol[claim_evidence_labels[claim_id][0]]\n","        elif len(set(claim_evidence_labels[claim_id])) == 2:\n","            if label_mapper_ltoi['NOT_ENOUGH_INFO'] in claim_evidence_labels[claim_id]:\n","                claim_labels[claim_id] = label_mapper_itol[(set(claim_evidence_labels[claim_id]) - {label_mapper_ltoi['NOT_ENOUGH_INFO']}).pop()]  # label as the other one: supports/refutes\n","            else:\n","                claim_labels[claim_id] = \"DISPUTED\"\n","        else:  # len(set(claim_evidence_labels[claim_id])) == 3\n","            claim_labels[claim_id] = \"DISPUTED\"\n","\n","    return claim_labels"],"metadata":{"id":"4vWkrPEm2n--","executionInfo":{"status":"ok","timestamp":1684144105035,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def evaluate_dev(net, dataloader, dev_claims, gpu):\n","    claim_labels = decide_claim_labels_majority_vote(net, dataloader, gpu)\n","\n","    correct_labels = 0\n","\n","    for claim_id in dev_claims:\n","        if claim_labels[claim_id] == dev_claims[claim_id][\"claim_label\"]:\n","            correct_labels += 1\n","    \n","    return correct_labels / len(dev_claims)  # claim label accuracy"],"metadata":{"id":"_AvJ9EFZEoK2","executionInfo":{"status":"ok","timestamp":1684144105035,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["net_clc = CFEVERLabelClassifier()\n","net_clc.cuda(gpu) #Enable gpu support for the model\n","\n","class_counts = defaultdict(int)\n","for cid in train_claims:\n","    if train_claims[cid]['claim_label'] != 'DISPUTED':\n","        class_counts[train_claims[cid]['claim_label']] += len(train_claims[cid]['evidences'])\n","\n","class_weights = torch.tensor([(sum(class_counts.values()) / class_counts[c]) for c in label_mapper_ltoi.keys()])\n","loss_criterion = nn.CrossEntropyLoss()#weight=class_weights).cuda(gpu)\n","opti_clc = optim.Adam(net_clc.parameters(), lr=opti_lr_clc)"],"metadata":{"id":"-HuWzz_wEp2N","executionInfo":{"status":"ok","timestamp":1684144106160,"user_tz":-600,"elapsed":1129,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["### **Claim Label Classification - Training**"],"metadata":{"id":"HFt5OiTzlvo1"}},{"cell_type":"markdown","source":["Integrate noise evidences that inherits the evidence retrival model biases from the preceding task:"],"metadata":{"id":"quu13jtT016V"}},{"cell_type":"code","source":["net_er.load_state_dict(torch.load(er_model_params_filename))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ryMuJE4f0QKh","executionInfo":{"status":"ok","timestamp":1683994374652,"user_tz":-600,"elapsed":2575,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}},"outputId":"51f1e33c-5a8b-4920-e75b-e5a4e78f4b87"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["Get predictions using the preceding ER model for training."],"metadata":{"id":"dbtd7oMkzQXQ"}},{"cell_type":"code","source":["test_train_set = CFEVERERTestDataset(train_claims, evidences, bert_tokenizer, max_candidates=100)\n","test_dev_set = CFEVERERTestDataset(dev_claims, evidences, bert_tokenizer, max_candidates=100)\n","\n","test_train_loader = DataLoader(test_train_set, batch_size=loader_batch_size, num_workers=loader_worker_num)\n","test_dev_loader = DataLoader(test_dev_set, batch_size=loader_batch_size, num_workers=loader_worker_num)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRUfCK_c0TuQ","executionInfo":{"status":"ok","timestamp":1683995721618,"user_tz":-600,"elapsed":1280190,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}},"outputId":"efa5def6-4d5c-4fe7-a466-3a66af38f81c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished unrolling test claim-evidence pairs in 1095.279700756073 seconds.\n","Finished unrolling test claim-evidence pairs in 184.49401569366455 seconds.\n"]}]},{"cell_type":"code","source":["train_claim_evidences = predict_evi(net_er, test_train_loader, gpu)\n","dev_claim_evidences = predict_evi(net_er, test_dev_loader, gpu)"],"metadata":{"id":"qHmtG4cj0Wz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clc_train_claims = copy.deepcopy(train_claims)\n","\n","for cid in clc_train_claims:\n","    clc_train_claims[cid]['evidences'].extend(train_claim_evidences[cid])\n","    clc_train_claims[cid]['evidences'] = list(set(clc_train_claims[cid]['evidences']))\n","\n","clc_dev_claims = copy.deepcopy(dev_claims)\n","\n","for cid in clc_dev_claims:\n","    clc_dev_claims[cid]['evidences'] = dev_claim_evidences[cid]"],"metadata":{"id":"pwWMjgT30YPU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the respective data loaders:"],"metadata":{"id":"kFnmf49gmE9z"}},{"cell_type":"code","source":["train_set = CFEVERLabelTrainDataset(clc_train_claims, evidences)\n","dev_set = CFEVERLabelTestDataset(clc_dev_claims, evidences)\n","\n","train_loader = DataLoader(train_set, batch_size=loader_batch_size, num_workers=loader_worker_num)\n","dev_loader = DataLoader(dev_set, batch_size=loader_batch_size, num_workers=loader_worker_num)"],"metadata":{"id":"DvsqxN_3EuuR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Do Training:"],"metadata":{"id":"Lx8gPYi5mHWk"}},{"cell_type":"code","source":["train_claim_cls(net_clc, loss_criterion, opti_clc, train_loader, dev_loader, clc_dev_claims, gpu)"],"metadata":{"id":"PixD7ubTE4oX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Claim Label Classification - Zero R Baseline**"],"metadata":{"id":"hQKFOsWwgwOx"}},{"cell_type":"code","source":["def zero_r_label_cls_baseline(train_claims, dev_claims):\n","    acc = 0\n","    majority_label = Counter([train_claims[c][\"claim_label\"] for c in train_claims]).most_common(1)[0][0]\n","\n","    for c in dev_claims:\n","        if dev_claims[c]['claim_label'] == majority_label:\n","            acc += 1\n","    \n","    return acc / len(dev_claims)"],"metadata":{"id":"xQA8ru5Pg2_p","executionInfo":{"status":"ok","timestamp":1684144107121,"user_tz":-600,"elapsed":3,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["acc = zero_r_label_cls_baseline(train_claims, dev_claims)\n","print(\"------Label Classification Baseline Performance------\")\n","print(f\"Accuracy: {acc}\")\n","print(\"-----------------------------------------------------\")"],"metadata":{"id":"xnhlMF3eg6oW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684144107121,"user_tz":-600,"elapsed":2,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}},"outputId":"e747b2c0-482b-4e34-bd3f-7ba0df2f058c"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["------Label Classification Baseline Performance------\n","Accuracy: 0.44155844155844154\n","-----------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### **Predict evidences and labels for test claims**"],"metadata":{"id":"fE7Cm0esKQOe"}},{"cell_type":"code","source":["output_filename = path_prefix + 'test-claims-predictions.json'"],"metadata":{"id":"5gG0aJa0KXTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_claim_evi_labels(test_claims, claim_labels):\n","    for claim in claim_labels:\n","        test_claims[claim][\"claim_label\"] = claim_labels[claim]\n","    \n","    with open(output_filename, 'w') as f:\n","        json.dump(test_claims, f)\n","    \n","    print(\"Final test claims predictions file ready.\")\n","    \n","    return test_claims"],"metadata":{"id":"nTv8pFtbKUAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net_er.load_state_dict(torch.load(er_model_params_filename))"],"metadata":{"id":"2UgN_-QlbjhG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","test_set_er = CFEVERERTestDataset(test_claims, evidences, bert_tokenizer)\n","test_loader_er = DataLoader(test_set_er, batch_size=loader_batch_size, num_workers=loader_worker_num)"],"metadata":{"id":"xny_SRVWLrrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_claims = extract_er_result(predict_evi(net_er, test_loader_er, gpu), test_claims)"],"metadata":{"id":"mqJQ186MMDQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net_clc.load_state_dict(torch.load(clc_model_params_filename))"],"metadata":{"id":"cnuK0Q2qKu1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684000872508,"user_tz":-600,"elapsed":848,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}},"outputId":"71fa7979-1944-489c-a9f8-9629b9ec9594"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["test_set_clc = CFEVERLabelTestDataset(test_claims, evidences)\n","test_loader_clc = DataLoader(test_set_clc, batch_size=loader_batch_size, num_workers=loader_worker_num)"],"metadata":{"id":"aY_3i85AKqXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["claim_labels = decide_claim_labels_majority_vote(net_clc, test_loader_clc, gpu)\n","extract_claim_evi_labels(test_claims, claim_labels)"],"metadata":{"id":"jwBenbXqKn7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **[Optional] Evidence Retrieval - Hard Negative Mining**"],"metadata":{"id":"t_KITCYfjBl3"}},{"cell_type":"code","source":["def unroll_train_claim_evidences_with_hne(claims, evidences_, claim_hard_negative_evidences, hne_sample_ratio=0.5):\n","    st = time.time()\n","\n","    train_claim_evidence_pairs = []\n","\n","    for claim in claims:\n","        for train_evidence_id, label in generate_random_train_evidence_samples(evidences_, claims[claim]['evidences'], hne_sample_ratio):\n","            train_claim_evidence_pairs.append((claim, train_evidence_id, label))\n","\n","        for train_evidence_id in claim_hard_negative_evidences[claim]:\n","            train_claim_evidence_pairs.append((claim, train_evidence_id, 0))\n","\n","    random.shuffle(train_claim_evidence_pairs)\n","    print(f\"Finished unrolling train claim-evidence pairs with hne in {time.time() - st} seconds.\")\n","\n","    return train_claim_evidence_pairs"],"metadata":{"id":"xN0PVArw84Qd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CFEVERERHNMDataset(Dataset):\n","    \"\"\"\n","    This dataset is used to obtain the hard negative evidences for a given claim\n","    for a pre-trained ER model. All evidences that are not positive for the claim\n","    are considered in the dataset.\n","\n","    Note: This dataset only takes one claim instead of all like in the normal train\n","    dataset above. Because hard negative evidences are selected for a claim at a time.\n","    \"\"\"\n","    def __init__(self, claim, evidences_, tokenizer, max_len=input_seq_max_len):\n","        self.data_set = [e for e in evidences_ if e not in claim['evidences']]  # get all negative samples\n","        self.max_len = max_len\n","        self.claim = claim\n","        self.evidences = evidences_\n","        self.target_hn_num = len(claim['evidences'])  # number of hard negative evidences to be selected\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, index):\n","        evidence_id = self.data_set[index]\n","\n","        # Preprocessing the text to be suitable for BERT\n","        claim_evidence_in_tokens = self.tokenizer.encode_plus(self.claim['claim_text'], self.evidences[evidence_id], \n","                                                              return_tensors='pt', padding='max_length', truncation=True,\n","                                                              max_length=self.max_len, return_token_type_ids=True)\n","        \n","        seq, attn_masks, segment_ids = claim_evidence_in_tokens['input_ids'].squeeze(0), claim_evidence_in_tokens[\n","                'attention_mask'].squeeze(0), claim_evidence_in_tokens['token_type_ids'].squeeze(0)\n","    \n","        return seq, attn_masks, segment_ids, evidence_id"],"metadata":{"id":"YIwzBIeLSC08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hnm(net, train_claims, evidences_, tokenizer, gpu, hnm_threshold=hnm_threshold, hnm_batch_size=hnm_batch_size):\n","    \"\"\"\n","    This function aims to select the hard negative evidences for each claim.\n","    returns a dict of claim_id -> list of hard negative evidences.\n","    \"\"\"\n","    net.eval()\n","    st = time.time()\n","\n","    claim_hard_negative_evidences = defaultdict(list)  # store the hard negative evidences for each claim\n","    \n","    for k, train_claim in enumerate(train_claims):  # for each claim in the training set\n","        test_train_set = CFEVERERHNMDataset(train_claims[train_claim], evidences_, tokenizer)  # get the dataset containing the negative evi for the claim\n","        test_train_loader = DataLoader(test_train_set, batch_size=hnm_batch_size, num_workers=loader_worker_num)\n","\n","        with torch.no_grad():  # suspend grad track, save time and memory\n","            for seq, attn_masks, segment_ids, evidence_ids in test_train_loader:  \n","                seq, attn_masks, segment_ids = seq.cuda(gpu), attn_masks.cuda(gpu), segment_ids.cuda(gpu)\n","                logits = net(seq, attn_masks, segment_ids)\n","                probs = get_probs_from_logits(logits)\n","\n","                indices = np.where(probs.cpu().numpy() > hnm_threshold)[0]  # get the indices of the hard negative evidences if any\n","                i = 0\n","\n","                while len(claim_hard_negative_evidences[train_claim]) < test_train_set.target_hn_num and i < len(indices):\n","                    \"\"\"While the number of hard negative evidences for the claim is less than the target number,\n","                    and there are still hard negative evidences in the indices, add the evidences to the list.\"\"\"\n","                    claim_hard_negative_evidences[train_claim].append(evidence_ids[indices[i]])\n","                    i += 1\n","\n","                if len(claim_hard_negative_evidences[train_claim]) == test_train_set.target_hn_num:  # if the enough hard negatives, break\n","                    break\n","        \n","        if k % 50 == 0:\n","            print(f\"{k}th claim finished in {time.time() - st} seconds.\")\n","            st = time.time()\n","    \n","    with open(claim_hard_negatives_filename, 'w') as f:\n","        json.dump(claim_hard_negative_evidences, f)\n","        print(\"\\nClaim hard negative evidences saved to file.\")\n","\n","    return claim_hard_negative_evidences"],"metadata":{"id":"mlxcRTPF8wYI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net_er.load_state_dict(torch.load(er_model_params_filename))  # load the best model\n","opti_er_hne = AdamW(net_er.parameters(), lr=opti_lr_er_hne, weight_decay=0.15)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IG3yj-HESHwP","executionInfo":{"status":"ok","timestamp":1684055523575,"user_tz":-600,"elapsed":1595,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}},"outputId":"4601ef81-3f34-487a-9332-a751e58feebc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#claim_hard_negative_evidences = json.load(open(claim_hard_negatives_filename, 'r'))\n","claim_hard_negative_evidences = hnm(net_er, train_claims, bert_tokenizer, evidences, gpu)"],"metadata":{"id":"vJN7jlPM9L36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set.reset_data_hne(claim_hard_negative_evidences)\n","train_loader = DataLoader(train_set, batch_size=loader_batch_size, num_workers=loader_worker_num)"],"metadata":{"id":"q3G_kJ3XMJ2E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evidence Retrieval - Training (Phrase 2)**"],"metadata":{"id":"M1PSthpKSgwZ"}},{"cell_type":"code","source":["train_evi_retrieval(net_er, loss_criterion, opti_er_hne, train_loader, dev_loader, dev_claims, gpu, num_epoch_hne, grad_step_period_hne)"],"metadata":{"id":"mrENgc2KSNw_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **[Optional] Claim Label Classification - Function Declarations | Alternative Version: Evidence Concatenation**"],"metadata":{"id":"Y24DZjhOMTiw"}},{"cell_type":"code","source":["clc_model_params_filename = path_prefix + 'cfeverlabelcls.dat'\n","\n","# ----------Hyperparameters of the entire pipeline----------\n","# --------------Claim Label Classification--------------\n","d_bert_base = 768\n","gpu = 0\n","input_seq_max_len = 512\n","loader_batch_size = 24\n","loader_worker_num = 2\n","num_epoch = 10\n","max_evi_num = 5\n","num_of_classes = 4\n","opti_lr_clc = 2e-5\n","label_mapper_ltoi = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n","label_mapper_itol = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO', 3: 'DISPUTED'}\n","# ------------------------------------------------------"],"metadata":{"id":"tYMVXcjwMkaK","executionInfo":{"status":"ok","timestamp":1684144134242,"user_tz":-600,"elapsed":568,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["class CFEVERLabelTrainDataset(Dataset):\n","    \"\"\"Climate Fact Extraction and Verification Dataset for Training, for the Evidence Retrival task.\"\"\"\n","\n","    def __init__(self, claims, evidences_, max_len=input_seq_max_len):\n","        self.data_set = [claims[c] for c in claims]\n","        self.max_len = max_len\n","        self.claims = claims\n","        self.evidences = evidences_\n","\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, index):\n","        claim = self.data_set[index]\n","\n","        claim_evidences = claim['evidences']\n","        random.shuffle(claim_evidences)\n","        evidences_combined = \" \".join([self.evidences[eid] for eid in claim_evidences])\n","\n","        # Preprocessing the text to be suitable for BERT\n","        claim_evidence_in_tokens = self.tokenizer.encode_plus(claim['claim_text'], evidences_combined, \n","                                                            return_tensors='pt', padding='max_length', truncation=True,\n","                                                            max_length=self.max_len, return_token_type_ids=True)\n","        \n","        seq, attn_masks, segment_ids = claim_evidence_in_tokens['input_ids'].squeeze(0), claim_evidence_in_tokens[\n","                'attention_mask'].squeeze(0), claim_evidence_in_tokens['token_type_ids'].squeeze(0)\n","\n","        return seq, attn_masks, segment_ids, label_mapper_ltoi[claim['claim_label']]"],"metadata":{"id":"vuDhf-6IMyVK","executionInfo":{"status":"ok","timestamp":1684144134828,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["class CFEVERLabelTestDataset(Dataset):\n","    \"\"\"Climate Fact Extraction and Verification Dataset for Testing, for the Evidence Retrival task.\"\"\"\n","\n","    def __init__(self, claims, evidences_, max_len=input_seq_max_len):\n","        self.data_set = [(c, claims[c]) for c in claims]\n","        self.max_len = max_len\n","        self.claims = claims\n","        self.evidences = evidences_\n","\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, index):\n","        claim_id, claim = self.data_set[index]\n","\n","        claim_evidences = claim['evidences']\n","        random.shuffle(claim_evidences)\n","        evidences_combined = \" \".join([self.evidences[eid] for eid in claim_evidences])\n","\n","        # Preprocessing the text to be suitable for BERT\n","        claim_evidence_in_tokens = self.tokenizer.encode_plus(claim['claim_text'], evidences_combined, \n","                                                            return_tensors='pt', padding='max_length', truncation=True,\n","                                                            max_length=self.max_len, return_token_type_ids=True)\n","        \n","        seq, attn_masks, segment_ids = claim_evidence_in_tokens['input_ids'].squeeze(0), claim_evidence_in_tokens[\n","                'attention_mask'].squeeze(0), claim_evidence_in_tokens['token_type_ids'].squeeze(0)\n","\n","        return seq, attn_masks, segment_ids, claim_id"],"metadata":{"id":"jSKKBM67Mzpl","executionInfo":{"status":"ok","timestamp":1684144134828,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["class CFEVERLabelClassifier(nn.Module):\n","    def __init__(self):\n","        super(CFEVERLabelClassifier, self).__init__()\n","\n","        # Instantiating BERT model object\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Classification layer\n","        # input dimension is 768 because [CLS] embedding has a dimension of 768, if bert base is used\n","        # output dimension is 1 because we're working with a binary classification problem - RELEVANT : NOT RELEVANT\n","        self.cls_layer = nn.Linear(d_bert_base, num_of_classes)\n","\n","    def forward(self, seq, attn_masks, segment_ids):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","            -segment_ids : Tensor of shape [B, T] containing token ids of segment embeddings (see BERT paper for more details)\n","        '''\n","        \n","        # Feeding the input to BERT model to obtain contextualized representations\n","        outputs = self.bert(seq, attention_mask=attn_masks, token_type_ids=segment_ids, return_dict=True)\n","        cont_reps = outputs.last_hidden_state\n","\n","        # Obtaining the representation of [CLS] head (the first token)\n","        cls_rep = cont_reps[:, 0]\n","\n","        # Feeding cls_rep to the classifier layer\n","        logits = self.cls_layer(cls_rep)\n","\n","        return logits  # logits shape is [B, num_of_classes]"],"metadata":{"id":"sMO9NbtzM02N","executionInfo":{"status":"ok","timestamp":1684144134828,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["def train_claim_cls(net, loss_criterion, opti, train_loader, dev_loader, dev_claims, gpu, max_eps=num_epoch):\n","    best_acc = 0\n","    mean_losses = [0] * max_eps\n","\n","    for ep in range(max_eps):\n","        net.train()  # Good practice to set the mode of the model\n","        st = time.time()\n","        train_acc = 0\n","        count = 0\n","        \n","        for i, (b_seq, b_attn_masks, b_segment_ids, b_label) in enumerate(train_loader):\n","            # Reset/Clear gradients\n","            opti.zero_grad()\n","\n","            # Extracting the tokens ids, attention masks and token type ids\n","            b_seq, b_attn_masks, b_segment_ids, b_label = b_seq.cuda(gpu), b_attn_masks.cuda(gpu), b_segment_ids.cuda(gpu), b_label.cuda(gpu)\n","\n","            # Obtaining the logits from the model\n","            logits = net(b_seq, b_attn_masks, b_segment_ids)\n","\n","            # Computing loss\n","            loss = loss_criterion(logits, b_label)\n","\n","            mean_losses[ep] += loss.item()\n","            count += 1\n","            train_acc += get_accuracy_from_logits(logits, b_label)\n","\n","            # Backpropagating the gradients, account for gradients\n","            loss.backward()\n","\n","            # Optimization step, apply the gradients\n","            opti.step()\n","\n","            if i % 100 == 0:\n","                print(\"Iteration {} of epoch {} complete. Time taken (s): {}\".format(i, ep, (time.time() - st)))\n","                st = time.time()\n","        \n","        mean_losses[ep] /= count\n","        print(f\"Epoch {ep} completed. Loss: {mean_losses[ep]}, Accuracy: {train_acc / count}.\")\n","\n","        dev_acc = evaluate_dev(net, dev_loader, dev_claims, gpu)\n","        print(\"\\nEpoch {} complete! Development Accuracy on dev claim labels: {}.\".format(ep, dev_acc))\n","        if dev_acc > best_acc:\n","            print(\"Best development accuracy improved from {} to {}, saving model...\\n\".format(best_acc, dev_acc))\n","            best_acc = dev_acc\n","            torch.save(net.state_dict(), clc_model_params_filename)\n","        else:\n","            print()\n","\n","    return mean_losses"],"metadata":{"id":"M8qopyADM1Eo","executionInfo":{"status":"ok","timestamp":1684144134828,"user_tz":-600,"elapsed":3,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["def get_accuracy_from_logits(logits, labels):\n","    probs = F.softmax(logits, dim=-1)\n","    predicted_classes = torch.argmax(probs, dim=1)\n","    acc = (predicted_classes.squeeze() == labels).float().mean()\n","    return acc\n","\n","\n","def get_predictions_from_logits(logits):\n","    probs = F.softmax(logits, dim=-1)\n","    predicted_classes = torch.argmax(probs, dim=1)\n","    return predicted_classes.squeeze()"],"metadata":{"id":"1FgrvrUrM6Fu","executionInfo":{"status":"ok","timestamp":1684144134829,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["def predict(net, dataloader, gpu):\n","    net.eval()\n","\n","    claim_labels = {}\n","    df = pd.DataFrame()\n","\n","    with torch.no_grad():\n","        for b_seq, b_attn_masks, b_segment_ids, b_claim_id in dataloader:\n","            b_seq, b_attn_masks, b_segment_ids, = b_seq.cuda(gpu), b_attn_masks.cuda(gpu), b_segment_ids.cuda(gpu)\n","            logits = net(b_seq, b_attn_masks, b_segment_ids)\n","\n","            preds = get_predictions_from_logits(logits)\n","            df = pd.concat([df, pd.DataFrame({'claim_ids': b_claim_id, 'preds': preds.cpu()})], ignore_index=True)\n","\n","    for _, row in df.iterrows():\n","        claim_id = row['claim_ids']\n","        label = row['preds']\n","\n","        claim_labels[claim_id] = label_mapper_itol[label]\n","    \n","    return claim_labels"],"metadata":{"id":"2NlcNtp4M7Z3","executionInfo":{"status":"ok","timestamp":1684144134829,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["def evaluate_dev(net, dataloader, dev_claims, gpu):\n","    claim_labels = predict(net, dataloader, gpu)\n","\n","    correct_labels = 0\n","\n","    for claim_id in dev_claims:\n","        if claim_labels[claim_id] == dev_claims[claim_id][\"claim_label\"]:\n","            correct_labels += 1\n","    \n","    return correct_labels / len(dev_claims)  # claim label accuracy"],"metadata":{"id":"iWslm0aMM8tr","executionInfo":{"status":"ok","timestamp":1684144134829,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["def extract_claim_evi_labels(test_claims, claim_labels, output_filename):\n","    for claim in claim_labels:\n","        test_claims[claim][\"claim_label\"] = claim_labels[claim]\n","    \n","    with open(output_filename, 'w') as f:\n","        json.dump(test_claims, f)\n","    \n","    print(\"Final test claims predictions file ready.\")\n","    \n","    return test_claims"],"metadata":{"id":"SfCxY7QLM-7L","executionInfo":{"status":"ok","timestamp":1684144134829,"user_tz":-600,"elapsed":4,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["net_clc = CFEVERLabelClassifier()\n","net_clc.cuda(gpu) # Enable gpu support for the model\n","\n","class_counts = Counter([train_claims[claim][\"claim_label\"] for claim in train_claims])\n","class_weights = torch.tensor([(sum(class_counts.values()) / class_counts[c]) for c in label_mapper_ltoi.keys()])\n","loss_criterion = nn.CrossEntropyLoss(weight=class_weights).cuda(gpu)\n","opti_clc = optim.Adam(net_clc.parameters(), lr=opti_lr_clc)"],"metadata":{"id":"_zyTNhFXOMNZ","executionInfo":{"status":"ok","timestamp":1684144136369,"user_tz":-600,"elapsed":1543,"user":{"displayName":"Yinghua Zhou","userId":"08764159452592421119"}}},"execution_count":47,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["N6JbKZxBlkMB","QvA3TgKsjYSl","EG0vd_xl4FI0","wg6GXDB3DzY2","HFt5OiTzlvo1","hQKFOsWwgwOx","fE7Cm0esKQOe","t_KITCYfjBl3","Y24DZjhOMTiw"],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d5f8aa6ac64d4250b4c970e2e44710bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed732b4df94b49fe87ad7517ee44f711","IPY_MODEL_0bfb603807b44e77ae8b0fc97408c678","IPY_MODEL_b3fb05286ae34af1ae2b7a58690e47f9"],"layout":"IPY_MODEL_432657e0c65c46a384e001e307761cde"}},"ed732b4df94b49fe87ad7517ee44f711":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12131d25b19d4eb189b3343ec7aef74c","placeholder":"​","style":"IPY_MODEL_dddd9ae8ebb34cea99543d2bccada370","value":"Downloading (…)lve/main/config.json: 100%"}},"0bfb603807b44e77ae8b0fc97408c678":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bed5c487b6247088ccc15bf5b8c1618","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6dde0e00ecc468fbb493243bac2e302","value":570}},"b3fb05286ae34af1ae2b7a58690e47f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55df7111524d4e588e82e04519ae0f01","placeholder":"​","style":"IPY_MODEL_16177e50dd164c318aff70ab0c873790","value":" 570/570 [00:00&lt;00:00, 24.0kB/s]"}},"432657e0c65c46a384e001e307761cde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12131d25b19d4eb189b3343ec7aef74c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dddd9ae8ebb34cea99543d2bccada370":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bed5c487b6247088ccc15bf5b8c1618":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6dde0e00ecc468fbb493243bac2e302":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55df7111524d4e588e82e04519ae0f01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16177e50dd164c318aff70ab0c873790":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"055934f932724136b887bc058dbe6c5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_821ffe1137e44e9f8c870a272de4e4ce","IPY_MODEL_755725ae4e3e49d1a700b993aff4dada","IPY_MODEL_228a374a85b8467d94ac93d2c7f554d8"],"layout":"IPY_MODEL_bdd9c9b2edcd493798bebdcc467db72c"}},"821ffe1137e44e9f8c870a272de4e4ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fa980f3162f4022ae04c1f2d26c8ee9","placeholder":"​","style":"IPY_MODEL_90e26c7cf5c446ab88a58d4c883e330b","value":"Downloading pytorch_model.bin: 100%"}},"755725ae4e3e49d1a700b993aff4dada":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_868a1c7c37f14047ab933a71c0c711ae","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9459649412942afa976eb59a9dd105a","value":440473133}},"228a374a85b8467d94ac93d2c7f554d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b192a06cddde4b25993035590e4963dd","placeholder":"​","style":"IPY_MODEL_ac5af82a4cbb44628c92beac868b79b9","value":" 440M/440M [00:02&lt;00:00, 235MB/s]"}},"bdd9c9b2edcd493798bebdcc467db72c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fa980f3162f4022ae04c1f2d26c8ee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90e26c7cf5c446ab88a58d4c883e330b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"868a1c7c37f14047ab933a71c0c711ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9459649412942afa976eb59a9dd105a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b192a06cddde4b25993035590e4963dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac5af82a4cbb44628c92beac868b79b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"302c6088874c479794d4ebbcc6c8626c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_849e55240f914a2baf69fe7cc96d7b47","IPY_MODEL_fb00b70c0dd44356b0c33465870e86a2","IPY_MODEL_22a5d51f83b34f659effb646f053938e"],"layout":"IPY_MODEL_5a97613001b845ab95f0416e253cfb5a"}},"849e55240f914a2baf69fe7cc96d7b47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67b8a743c6db4e419346d17c2c492f32","placeholder":"​","style":"IPY_MODEL_7b3a68edd4904e269163144038e5f224","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"fb00b70c0dd44356b0c33465870e86a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f13259a798124913a6cf9b3c7de47952","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70a672b5e8d6444a8837b13763ba31a1","value":231508}},"22a5d51f83b34f659effb646f053938e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfe6367e4bb445d580b0d6d742bd5943","placeholder":"​","style":"IPY_MODEL_bf654a93dc134c27b13adc88bd3488dd","value":" 232k/232k [00:00&lt;00:00, 3.38MB/s]"}},"5a97613001b845ab95f0416e253cfb5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67b8a743c6db4e419346d17c2c492f32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b3a68edd4904e269163144038e5f224":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f13259a798124913a6cf9b3c7de47952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70a672b5e8d6444a8837b13763ba31a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfe6367e4bb445d580b0d6d742bd5943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf654a93dc134c27b13adc88bd3488dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f44557afc58544879652587334309b83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1cb9e6592a674e0fb180413aec800865","IPY_MODEL_3d4e34f20ba647f09312420f90090903","IPY_MODEL_26eac9f552e546eaa6245d9a52960e35"],"layout":"IPY_MODEL_c5d982e03c3e4341aa2d225f3c542102"}},"1cb9e6592a674e0fb180413aec800865":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0635dc12c8e6410cbe0f40d8b599c89e","placeholder":"​","style":"IPY_MODEL_8f5d1be7d02044e0b52787247edd8b3e","value":"Downloading (…)okenizer_config.json: 100%"}},"3d4e34f20ba647f09312420f90090903":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f099b02528734f0d99a049741ce4ff39","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c7dab5b9ca34421bdc545bf9f0d4a0c","value":28}},"26eac9f552e546eaa6245d9a52960e35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da03bf37767403ea368474ddaeb1ccf","placeholder":"​","style":"IPY_MODEL_64941ef4cf63481eb7338b6999cfaec3","value":" 28.0/28.0 [00:00&lt;00:00, 959B/s]"}},"c5d982e03c3e4341aa2d225f3c542102":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0635dc12c8e6410cbe0f40d8b599c89e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f5d1be7d02044e0b52787247edd8b3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f099b02528734f0d99a049741ce4ff39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c7dab5b9ca34421bdc545bf9f0d4a0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7da03bf37767403ea368474ddaeb1ccf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64941ef4cf63481eb7338b6999cfaec3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}